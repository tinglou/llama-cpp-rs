[package]
name = "llama-cpp-sys-2"
description = "Low Level Bindings to llama.cpp"
version = "0.1.103"
edition = "2021"
license = "MIT OR Apache-2.0"
repository = "https://github.com/utilityai/llama-cpp-rs"
links = "llama"

include = [
    "wrapper.h",
    "build.rs",
    "/src",

    "/llama.cpp/common/*.h",
    "/llama.cpp/common/*.hpp",
    "/llama.cpp/common/*.cpp",
    "/llama.cpp/ggml/include/*.h",
    "/llama.cpp/ggml/src/*.h",
    "/llama.cpp/ggml/src/*.c",
    "/llama.cpp/ggml/src/*.cpp",
    "/llama.cpp/src/*.h",
    "/llama.cpp/src/*.cpp",

    "/llama.cpp/convert_hf_to_gguf.py", # Yes, it's required

    # Erroneously the llama.cpp code currently generates the build-info.cpp
    # into the source directory of the build instead of into the target directory
    # as it should. Will try submitting something upstream to clean this up as
    # well but for now explictly exclude this from the build. Previously this was
    # implicitly excluded because the llama.cpp code was copied wholesale into the
    # target directory for building which is why this problem wasn't visible before
    # (i.e. we'd package the llama.cpp source from the submodule & thus this build-info.cpp
    # generated file would still be ignored because it would only exist in the separate
    # copy within the target directory. An alternative, if we do want to capture build-info.cpp
    # within the package would be to change the CI task to add `--allow-dirty` to the package
    # command.
    "!/llama.cpp/common/build-info.cpp",
    "/llama.cpp/common/build-info.cpp.in",

    "/llama.cpp/ggml/src/ggml-cuda.cu",
    "/llama.cpp/ggml/src/ggml-metal.m",
    "/llama.cpp/ggml/src/ggml-metal.metal",

    "/llama.cpp/include/llama.h",
    "/llama.cpp/include/llama-cpp.h",

    "/llama.cpp/ggml/src/ggml-cpu/**/*",
    "/llama.cpp/ggml/src/ggml-cuda/**/*",
    "/llama.cpp/ggml/src/ggml-metal/**/*",
    "/llama.cpp/ggml/src/ggml-vulkan/**/*",

    "/llama.cpp/ggml/src/llamafile/sgemm.h",
    "/llama.cpp/ggml/src/llamafile/sgemm.cpp",

    "/llama.cpp/pocs",
    "/llama.cpp/examples/**/*",

    "/llama.cpp/CMakeLists.txt",
    "/llama.cpp/common/CMakeLists.txt",
    "/llama.cpp/ggml/CMakeLists.txt",
    "/llama.cpp/ggml/src/CMakeLists.txt",
    "/llama.cpp/src/CMakeLists.txt",

    "/llama.cpp/cmake",
    "/llama.cpp/ggml/cmake",
    "/llama.cpp/common/cmake",
]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]

[build-dependencies]
bindgen = { workspace = true }
cc = { workspace = true, features = ["parallel"] }
cmake = "0.1"
find_cuda_helper = "0.2.0"
glob = "0.3.2"
anyhow = "1.0"
walkdir = "2"

[features]
cuda = []
# Disables the need to dynamically link against libcuda.so / cuda.dll
cuda-no-vmm = ["cuda"]
metal = []
dynamic-link = []
vulkan = []
native = []
openmp = []
sycl-f16 = ["dynamic-link"] # faster
sycl-f32 = ["dynamic-link"] # better quality
kompute = []
rpc = []
opencl = []
# Only has an impact on Android.
shared-stdcxx = []

ggml-cpu-hbm = []
ggml-cpu-aarch64 = []
ggml-avx = []
ggml-avx-vnni = []
ggml-avx2 = []
ggml-avx512 = []
ggml-avx512-vbmi = []
ggml-avx512-vnni = []
ggml-avx512-bf16 = []
# in MSVC F16C and FMA is implied with AVX2/AVX512
ggml-fma = []
ggml-f16c = []
# MSVC does not seem to support AMX
ggml-amx-tile = []
ggml-amx-int8 = []
ggml-amx-bf16 = []
ggml-lasx = []
ggml-lsx = []
ggml-rvv = []
